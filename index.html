<!DOCTYPE html>
<html lang="en">
<head>
	<!--Import Google Icon Font-->
	<link href="http://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
	<!--Import materialize.css-->
	<link type="text/css" rel="stylesheet" href="css/materialize.min.css"  media="screen,projection"/>
	<link type="text/css" rel="stylesheet" href="css/style.css" media="screen,projection"/>

	<!--Let browser know website is optimized for mobile-->
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no"/>
	<title>Matteo Pirotta</title>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90538549-1', 'auto');
  ga('send', 'pageview');

</script>
</head>

<body>
<div class=" blue darken-1">
<!-- photo -->
<div class="container">
	<h1 class="header center-on-small-only">Matteo Pirotta</h1>
	<div class="row center">
            <!--<h4 class="header col s12 light center">Postdoc researcher in machine learning at INRIA</h4>-->
            <h4 class="header col s12 light center">Postdoc researcher in <i>machine learning</i> at INRIA</h4>
  </div>
</div>
	<!-- NAVIGATION BAR -->
	<nav class="blue darken-3" role="navigation">
		<div class="nav-wrapper container">
			<!--<a id="logo-container" href="#" class="brand-logo white-text"><span class="pubs_me">Matteo Pirotta</span></a>-->
			<ul class="right hide-on-med-and-down">
				<li><a class="white-text" href="#about">About Me</a></li>
				<li><a class="white-text" href="#pubs">Publications</a></li>
				<li><a class="white-text" href="#teaching">Teaching</a></li>
				<li><a class="white-text" href="#foot">Links</a></li>
				<li><a class="white-text" href="cv_mpirotta.pdf">R&eacute;sum&eacute;</a></li>
			</ul>

			<ul id="nav-mobile" class="side-nav">
				<li><a href="#about">About Me</a></li>
				<li><a href="#pubs">Publications</a></li>
				<li><a href="#teaching">Teaching</a></li>
				<li><a href="#foot">Links</a></li>
				<li><a href="cv_mpirotta.pdf">Resume</a></li>
			</ul>
			<a href="#" data-activates="nav-mobile" class="button-collapse"><i class="material-icons">menu</i></a>
		</div>
	</nav>
</div>
	<!-- CONTAINER -->
	<div class="container">
		<div class="section" id="about">
			<!--   About Me Section   -->
			<div class="row ">
				<div class="col s12 m8 offset-l1 l6">
					<!--<h2 class="center brown-text"><i class="material-icons">flash_on</i></h2>-->
					<h4 class="center">About Me</h4>

					<p class="light text-justify">
						I am currently a postdoc at <a href="https://www.inria.fr/en/centre/lille">INRIA Lille - Nord Europe</a> in the SequeL team (under <a href="http://researchers.lille.inria.fr/~lazaric/">Alessandro Lazaric</a>). Previously, I was a postdoc at <a href="http://www.polimi.it/">Politecnico di Milano</a> and before a machine learning scientist at UniCredit.
						Before I have been PhD student in computer science at Politecnico di Milano, under the supervision of
						<a href="http://bascetta.deib.polimi.it/index.php/Main_Page">Luca Bascetta</a> and <a href="http://home.deib.polimi.it/restelli/MyWebSite/index.shtml">Marcello Restelli</a>.
					</p>
					<p class="light text-justify">
						My research interest is machine learning. In particular I am interested in <b>reinforcement learning</b>, transfer learning and <i>online learning</i>.
						I am particularly interested in financial applications involving, for example, portfolio management.
					</p>
					<p class="light text-justify">
						More details in my <a href="cv_mpirotta.pdf">CV</a>.
					</p>
					<p class="light text-justify">
						<b>Contacts:</b><br>
						Email-1: 	matteo DOT pirotta AT inria DOT fr<br>
						Email-2: 	matteo DOT pirotta AT polimi DOT it
					</p>
					<p class="light text-justify">
						<b>Github:</b><br>
						<a href="https://github.com/teopir/">https://github.com/teopir</a>
					</p>
				</div>
					<div class="col s12 m3 offset-l1 l3 valign center" style="padding-top:10px">
						<img src="pics/me_castel_lake_squared.jpg" alt="" class="z-depth-2" width="440">
					</div>
			</div>
		</div>
		<div class="divider"></div>
		<div class="section" id="pubs">
			<!--   Publications Section   -->
			<div class="row light">
				<div class="col s12 m12 offset-l1 l10">
					<h4 class="center">Publications</h4>
					<h5>Journal Papers</h5>
					<ul class="pubs_ul">
						<li>
							Simone Parisi, <span class="pubs_me">Matteo Pirotta</span> and Jan Peters:<br>
							<span class="pubs_title">Manifold-based Multi-objective Policy Search with Sample Reuse.
							</span>
							Neurocomputing 263, 2017.
						</li>
						<li>
							Giorgio Manganini, <span class="pubs_me">Matteo Pirotta</span>, Marcello Restelli, Luigi Piroddi, and
							Maria Prandini:<br>
							<span class="pubs_title">Policy search for the optimal control of Markov decision processes:
								a novel particle-based iterative scheme.
							</span>
							IEEE Transactions on Cybernetics 46, 2016.
						</li>
						<li>
							Simone Parisi, <span class="pubs_me">Matteo Pirotta</span> and Marcello Restelli:<br>
							<span class="pubs_title">Multi-objective Reinforcement Learning through Continuous
								Pareto Manifold Approximation.
							</span>
							Journal of Artificial Intelligence Research 57, 2016.
						</li>
						<li>
							<span class="pubs_me">Matteo Pirotta</span>, Marcello Restelli and Luca Bascetta:<br>
							<span class="pubs_title">Policy Gradient in Lipschitz Markov Decision Processes.</span>
							Machine Learning 100, 2015.
						</li>
					</ul>
					<h5>Conference Papers</h5>
					<ul class="pubs_ul">
						<li>Ronan Fruit, <span class="pubs_me">Matteo Pirotta</span>, Alessandro Lazaric, and Emma Brunskill:<br>
							<span class="pubs_title">Regret Minimization in MDPs with Options without Prior Knowledge.
							</span>
                            NIPS 2017, Long Beach, California, USA.
														[<a href="http://fruit.nom.fr/WordPress3/wp-content/uploads/2016/12/poster_options.pdf">Poster</a>]
						</li>
						<li>Alberto Metelli, <span class="pubs_me">Matteo Pirotta</span>, and Marcello Restelli:<br>
							<span class="pubs_title">Compatible Reward Inverse Reinforcement Learning.
							</span>
                            NIPS 2017, Long Beach, California, USA.
														[<a href="https://albertometelli.github.io/download/poster_nips2017.pdf">Poster</a>]
						</li>
						<li>Matteo Papini, <span class="pubs_me">Matteo Pirotta</span>, and Marcello Restelli:<br>
							<span class="pubs_title">Adaptive Batch Size for Safe Policy Gradients.
							</span>
                            NIPS 2017, Long Beach, California, USA.
														[<a href="https://t3p.github.io/download/poster_NIPS17.pdf">Poster</a>]
						</li>
						<li>Davide Tateo, <span class="pubs_me">Matteo Pirotta</span>, Andrea Bonarini and Marcello Restelli:<br>
							<span class="pubs_title">Gradient-Based Minimization for Multi-Expert Inverse Reinforcement Learning.
							</span>
                            IEEE SSCI 2017, Hawaii, USA.
						</li>
						<li>Samuele Tosatto, <span class="pubs_me">Matteo Pirotta</span>, Carlo D'Eramo, and Marcello Restelli:<br>
							<span class="pubs_title">Boosted Fitted Q-Iteration.
							</span>
                            ICML 2017, Sydney, New South Wales, Australia.
						</li>
						<li>Carlo D'Eramo, Alessandro Nuara, <span class="pubs_me">Matteo Pirotta</span>, and Marcello Restelli:<br>
							<span class="pubs_title">Estimating the Maximum Expected Value in Continuous Reinforcement Learning Problems.
							</span>
							AAAI 2017, San Francisco, California, USA.
						</li>
						<li><span class="pubs_me">Matteo Pirotta</span>, and Marcello Restelli:<br>
							<span class="pubs_title">Inverse Reinforcement Learning through Policy Gradient Minimization.
							</span>
							AAAI 2016, Phoenix, Arizona, USA.
						</li>
						<li><span class="pubs_me">Matteo Pirotta</span>, Simone Parisi, and Marcello Restelli:<br>
							<span class="pubs_title">Multi-Objective Reinforcement Learning with Continuous Pareto Frontier Approximation.
							</span>
							AAAI 2015, Austin, Texas, USA.
						</li>
						<li>Caporale Danilo, Luca Deori, Roberto Mura, Alessandro Falsone, Riccardo Vignali, Luca Giulioni,
							<span class="pubs_me">Matteo Pirotta</span> and Giorgio Manganini:<br>
							<span class="pubs_title">Optimal Control to Reduce Emissions in Gasoline
								Engines: An Iterative Learning Control Approach for ECU Calibration Maps Improvement.
							</span>
							ECC 2015, Linz, Austria.
						</li>
						<li>Giorgio Manganini, <span class="pubs_me">Matteo Pirotta</span>, Marcello Restelli, Luca Bascetta:<br>
							<span class="pubs_title">Following Newton
								Direction in Policy Gradient with Parameter Exploration.
							</span>
							IJCNN 2015, Killarney, Ireland.
						</li>
						<li>Simone Parisi, <span class="pubs_me">Matteo Pirotta</span>, Nicola Smacchia, Luca Bascetta, Marcello Restelli:<br>
							<span class="pubs_title">Policy
								Gradient Approaches for Multi-Objective Sequential Decision Making: A Comparison.
							</span>
							ADPRL 2014, Orlando, Florida, United States.
						</li>
						<li>Simone Parisi, <span class="pubs_me">Matteo Pirotta</span>, Nicola Smacchia, Luca Bascetta and Marcello Restelli:<br>
							<span class="pubs_title">Policy
								Gradient Approaches for Multi-Objective Sequential Decision Making.
							</span>
							IJCNN 2014, Beijing, China.
						</li>
						<li><span class="pubs_me">Matteo Pirotta</span>, Giorgio Manganini, Luigi Piroddi, Maria Prandini and Marcello Restelli:<br>
							<span class="pubs_title">A particle-based policy for the optimal control of Markov decision processes.</span>
							IFAC 2014, Cape Town, South Africa.
						</li>
						<li><span class="pubs_me">Matteo Pirotta</span>, Marcello Restelli, Luca Bascetta:<br>
							<span class="pubs_title">Adaptive Step-Size for Policy Gradient Methods.
							</span>
							NIPS 2013, Lake Tahoe, Nevada, USA.
						</li>
						<li><span class="pubs_me">Matteo Pirotta</span>, Marcello Restelli, Alessio Pecorino, and Daniele Calandriello:<br>
							<span class="pubs_title">Safe policy 	iteration.</span>
							ICML 2013, Atlanta, Georgia, USA.
						</li>
						<li>Martino Migliavacca, Alessio Pecorino, <span class="pubs_me">Matteo Pirotta</span>,
							Marcello Restelli, and Andrea Bonarini:<br>
							<span class="pubs_title">
								Fitted Policy Search.
							</span>
							ADPRL 2011, Paris, France.
						</li>
						<li>Martino Migliavacca, Alessio Pecorino, <span class="pubs_me">Matteo Pirotta</span>, Marcello Restelli, and Andrea Bonarini:<br>
							<span class="pubs_title">
								Fitted Policy Search: Direct Policy Search using a Batch Reinforcement Learning Approach.
							</span>
							ERLARS 2010, Lisboa, Portugal.
						</li>
					</ul>
					<h5>Workshops Papers</h5>
					<ul class="pubs_ul">
						<li>Ronan Fruit, <span class="pubs_me">Matteo Pirotta</span>, Alessandro Lazaric and Emma Brunskill:<br>
							<span class="pubs_title">
								Regret Minimization in MDPs with Options without Prior Knowledge.
							</span>
							<a href="http://rlabstraction2016.wixsite.com/icml-2017">Lifelong Learning: A Reinforcement Learning Approach</a>, ICML 2017 Workshop, Sydney, Australia.
							[<a href="https://drive.google.com/file/d/0B9dqzboiV5u-Z2h0cHVWa2JwWUk/view">PDF</a>, <a href="./papers/2017/ICML_LLL_options.pdf">PDF2</a>]
						</li>
						<li><span class="pubs_me">Matteo Pirotta</span>, and Marcello Restelli:<br>
							<span class="pubs_title">
								Cost-Sensitive Approach for Batch Size Optimization.
							</span>
							<a href="http://probabilistic-numerics.org/meetings/NIPS2016/">Optimizing the optimizers</a>, NIPS 2016 Workshop, Barcelona, Spain.
							[<a href="http://probabilistic-numerics.org/assets/pdf/NIPS2016/Pirotta_Restelli.pdf">PDF</a>]
						</li>
					</ul>
				</div>
			</div>
		</div><!-- section pubs -->
		<div class="divider"></div>
		<div class="section" id="teaching">
			<!--   Teaching Section   -->
			<div class="row light">
				<div class="col s12 m12 offset-l1 l10">
					<h4 class="center">Teaching</h4>
					<h5>MVA - Reinforcement Learning 2017</h5>
					<ul class="pubs_ul">
						<li> [2017-10-24] Dynamic Programming and Reinforcement Learning</br>
							<b>Homework:</b> <a href="teaching/MVA_RL_17/homework1.pdf">text</a></br>
							<b>Code:</b>
							<a href="teaching/MVA_RL_17/TP1_python.zip">Python 2.7/3.6</a>,
							<a href="teaching/MVA_RL_17/TP1_matlab.zip">Matlab (new)</a> (for python 2.7 you should install <i>future</i> library)
							<p>
								<span class="red-text text-darken-4"> The NEW deadline is November 12.</span>
							</p>
							<p>
								<span class="red-text text-darken-4">[Updated 2017-10-26 11:50]</span> fixed error in the Matlab code. The list of available actions for each state was not correctly computed.
							</p>
							<p>
								<span class="red-text text-darken-4">[Updated 2017-10-24 15:30]</span> added new python 2.7 version of the code and fixed typos in the pdf. Note that the graphic rendering in python requires <i>tkinter</i> (check the README.txt).</br></br>
								Question Q4: the fact that the policy is deterministic does not imply that the Q-function is zero for actions not selected. If you obtain a Q-function that is different from the provided one is just because you have never tested actions not selected by the policy (check that \(N(x,b) =0\) for any \(b \neq pi(x)\)). You can try to select \(a_0\) uniformly in the set of available actions in \(A(x)\) in order to obtain a "complete" Q-function.</br>
								Note that I am also asking to implement a simplified first-visit MC that exploits only the first pair \((x,a)\) of each trajectoy. If you are interested, a complete overview of MC estimators in RL can be found <a href="http://chercheurs.lille.inria.fr/~ghavamza/RL-EC-Lille/Lecture5-b.pdf">here</a>.
							</p>
						</li>
						<li> [2017-11-16] Exploration-Exploitation Dilemma (Stochastic Multi-Armed Bandit)</br>
							<b>Homework:</b> <a href="teaching/MVA_RL_17/homework2.pdf">text</a></br>
							<b>Code:</b>
							<a href="teaching/MVA_RL_17/TP2_python.zip">Python 2.7/3.6</a>,
							<a href="teaching/MVA_RL_17/TP2_matlab.zip">Matlab</a>
						</li>
						<p>
							<span class="red-text text-darken-4">[Updated 2017-11-20 12:30]</span> Updated mean of ExpArm.
						</p>
						<li> [2017-12-04] Reinforcement Learning with Function Approximation</br>
							<b>Homework:</b> <a href="teaching/MVA_RL_17/homework3_v1.pdf">text</a></br>
							<b>Code:</b>
							<a href="teaching/MVA_RL_17/TP3_python.zip">Python 2.7/3.6</a>,
							<a href="teaching/MVA_RL_17/TP3_matlab.zip">Matlab</a>
						</li>
						<p>
							<span class="red-text text-darken-4">[Updated 2017-12-12 11:20]</span> Updated matlab version to fix errors in <i>ConstantStep</i> and <i>estimate_performance</i>.
						</p>
						<p>
							<span class="red-text text-darken-4">[Updated 2017-12-05 19:30]</span> Note that the version you have to solve is <i>v1</i>.
						</p>
				</ul>
				</div>
			</div>
		</div>
	</div><!-- container -->

	<footer class="page-footer blue darken-3" id="foot">
		<div class="container">
			<div class="row">
				<div class="col offset-l1 l6 s8">
					<h5 class="white-text">Office</h5>
					<p class="grey-text text-lighten-4">
                        INRIA Lille - Nord Europe<br>
                        SequeL Team<br>
                        40 Avenue du Halley,<br>
                        59650 Villeneuve-d'Ascq, France
						<!--Politecnico di Milano<br>
						Dipartimento di Elettronica, Informazione e Bioingegneria (DEIB)<br>
						Via Ponzio 34/5<br>
						20133 Milano Italy-->
					</p>
				</div>
				<div class="col offset-l2 l3 s4">
					<h5 class="white-text">Links</h5>
					<ul>
						<li><a class="white-text" target="_blank" href="https://it.linkedin.com/in/matteo-pirotta-4593a994">Linkedin</a></li>
						<li><a class="white-text" target="_blank" href="https://scholar.google.com/citations?user=6qWcDTAAAAAJ&hl=en">Google Scholar</a></li>
						<li><a class="white-text" target="_blank" href="http://arxiv.org/find/grp_cs/1/au:+pirotta_matteo/0/1/0/all/0/1">Arxiv</a></li>
					</ul>
				</div>
			</div>
		</div>
		<div class="footer-copyright">
			<div class="container">
				<div class="row">
					<div class="col offset-l1 l5 s6">
						<p class="left">
							Powered by <a class="grey-text text-lighten-3" href="http://materializecss.com">Materialize</a>
						</p>
					</div>
					<div class="col l5 s6">
						<p class="right">
							Hosted on <a class="grey-text text-lighten-4" href="https://pages.github.com/">GitHub Pages</a>
						</p>
					</div>
				</div>
			</div>
		</div>
	</footer>




	<!--Import jQuery before materialize.js-->
	<script type="text/javascript" src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
	<script type="text/javascript" src="js/materialize.min.js"></script>
	<script src="js/init.js"></script>
	<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
	</script>
</body>
</html>
