<!DOCTYPE html>
<html lang="en">
    <head>
        <!--Import Google Icon Font-->
        <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=Handlee" rel="stylesheet">

        <!--Import materialize.css-->
        <link type="text/css" rel="stylesheet" href="css/materialize.min.css"  media="screen"/>

        <link type="text/css" rel="stylesheet" href="css/style.css" media="screen"/>
        <link type="text/css" rel="stylesheet" href="css/icon_fonts.css" media="screen"/>

        <link rel="icon" type="image/png" href="pics/g922.png">

        <!-- awesome icons -->
        <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.1.0/css/all.css" integrity="sha384-lKuwvrZot6UHsBSfcMvOkWwlCMgc0TaWr+30HWe3a4ltaBwTZhyTEggF5tJv8tbt" crossorigin="anonymous">

        <!--Let browser know website is optimized for mobile-->
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
        <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no"/>
        <title>Matteo Pirotta</title>
        <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
                                    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

            ga('create', 'UA-90538549-1', 'auto');
            ga('send', 'pageview');

        </script>
    </head>

    <body>
        <div class=" blue darken-1">
            <!-- photo -->
            <div class="container">
                <h1 class="header center-on-small-only special-font">Matteo Pirotta</h1>
                <div class="row center">
                    <h4 class="header col s12 light center">Research Scientist at Facebook AI Research <i class="fab fa-facebook"></i></h4>
                    <!-- <h4 class="header col s12 light center">Postdoc researcher in <i>machine learning</i> at INRIA</h4> -->
                </div>
            </div>
            <!-- NAVIGATION BAR -->
            <nav class="blue darken-3">
                <div class="nav-wrapper container">
                    <!--<a id="logo-container" href="#" class="brand-logo white-text"><span class="pubs_me">Matteo Pirotta</span></a>-->
                    <ul class="right hide-on-med-and-down">
                        <li><a class="white-text" href="#about">About Me</a></li>
                        <li><a class="white-text" href="#news">News</a></li>
                        <li><a class="white-text" href="#pubs">Publications</a></li>
                        <li><a class="white-text" href="#teaching">Teaching</a></li>
                        <li><a class="white-text" href="#foot">Links</a></li>
                        <li><a class="white-text" href="cv_mpirotta.pdf">R&eacute;sum&eacute;</a></li>
                    </ul>

                    <ul id="nav-mobile" class="side-nav">
                        <li><a href="#about">About Me</a></li>
                        <li><a href="#news">News</a></li>
                        <li><a href="#pubs">Publications</a></li>
                        <li><a href="#teaching">Teaching</a></li>
                        <li><a href="#foot">Links</a></li>
                        <li><a href="cv_mpirotta.pdf">R&eacute;sum&eacute;</a></li>
                    </ul>
                    <a href="#" data-activates="nav-mobile" class="button-collapse"><i class="material-icons">menu</i></a>
                </div>
            </nav>
        </div>
        <!-- CONTAINER -->
        <div class="container">
            <div class="section" id="about">
                <!--   About Me Section   -->
                <div class="row ">
                    <div class="col s12 m7 offset-l1 l5">
                        <!--<h2 class="center brown-text"><i class="material-icons">flash_on</i></h2>-->
                        <h4 class="center">About Me</h4>

                        <p class="light text-justify">
                            I am research scientist at <a href="https://research.fb.com/category/facebook-ai-research/">Facebook AI Research</a> in Paris.
                            Previously, I was postdoc at <a href="https://www.inria.fr/en/centre/lille">INRIA Lille - Nord Europe</a> in the SequeL team for almost two years.
                            Before I was postdoc at <a href="http://www.polimi.it/">Politecnico di Milano</a>.
                            I have received my PhD in computer science at Politecnico di Milano, under the supervision of
                            <a href="http://bascetta.deib.polimi.it/index.php/Main_Page">Luca Bascetta</a> and <a href="http://home.deib.polimi.it/restelli/MyWebSite/index.shtml">Marcello Restelli</a>.
                            <!-- I am currently a postdoc at <a href="https://www.inria.fr/en/centre/lille">INRIA Lille - Nord Europe</a> in the SequeL team (working with <a href="http://researchers.lille.inria.fr/~lazaric/">Alessandro Lazaric</a>). Previously, I was a postdoc at <a href="http://www.polimi.it/">Politecnico di Milano</a> and before a machine learning scientist at UniCredit.
I have been PhD student in computer science at Politecnico di Milano, under the supervision of
<a href="http://bascetta.deib.polimi.it/index.php/Main_Page">Luca Bascetta</a> and <a href="http://home.deib.polimi.it/restelli/MyWebSite/index.shtml">Marcello Restelli</a>.-->
                        </p>
                        <p class="light text-justify">
                            My research interest is machine learning. In particular I am interested in <b>reinforcement learning</b>, transfer learning and <i>online learning</i>.
                            <!--I am particularly interested in financial applications involving, for example, portfolio management.-->
                        </p>
                        <p class="light text-justify">
                            More details in my <a href="cv_mpirotta.pdf">CV</a>.
                        </p>
                        <p class="light text-justify">
                            <i class="fas fa-envelope"></i> <b>Contacts:</b><br>
                            Email: matteo DOT pirotta AT gmail.com
                            <!-- Email-1: 	matteo DOT pirotta AT inria DOT fr<br> -->
                            <!-- Email-2: 	matteo DOT pirotta AT polimi DOT it -->
                        </p>
                        <p class="light text-justify">
                            <i class="fab fa-github"></i> <b>Github:</b><br>
                            <a href="https://github.com/teopir/">https://github.com/teopir</a>
                        </p>
                    </div>
                    <div class="col s12 m5 offset-l1 l5 valign center">
                        <img src="pics/me_castel_lake_squared.jpg" alt="" class="responsive-img z-depth-2">
                    </div>
                </div>
                <!--   TALKS   -->
                <!-- <div class="row ">
<div class="col s12 m12 offset-l1 l10">
<h5>Talks</h5>
<ul class="pubs_ul">
<li>
<span class="news_date">[Apr 27, 2018]</span> Google Zurich (<i>Exploration-Exploitation in RL</i>)
</li>
<li>
<span class="news_date">[Apr 17, 2018]</span> Facebook Paris (<i>Exploration-Exploitation in RL</i>)
</li>
<li>
<span class="news_date">[Apr 03, 2018]</span> Politecnico di Milano (<i>Exploration-Exploitation in RL</i>)
</li>
<li>
<span class="news_date">[Mar 19, 2018]</span> Amazon Berlin (<i>Efficient Exploration-Exploitation in RL</i>)
</li>
<li>
<span class="news_date">[Jul 14, 2017]</span> UC Berkeley (<i>Regret Minimization in MDPs with Options</i>)
</li>
</ul>
</div>
</div> -->
                <!--   NEWS   -->
                <div class="row ">
                    <div class="col s12 m12 offset-l1 l10">
                        <h5>Selected Publications</h5>
                        <ul class="pubs_ul">
                            <li>
                                Yonathan Efroni, Shie Mannor and <span class="pubs_me">Matteo Pirotta</span>:<br>
                                <span class="pubs_title">Exploration-Exploitation in Constrained MDPs.
                                </span> arXiv:2003.02189, 2020. [<a href="https://arxiv.org/abs/2003.02189">arXiv</a>]
                            </li>
                            <li>
                                Ronan Fruit, <span class="pubs_me">Matteo Pirotta</span> and Alessandro Lazaric:<br>
                                <span class="pubs_title">Improved Analysis of UCRL2 with empirical Bernstein bounds.
                                </span> ALT Tutorial, 2019. [<a href="https://rlgammazero.github.io/docs/ucrl2b_improved.pdf">arXiv</a>]
                            </li>
                        </ul>
                    </div>
                </div>
            </div> <!-- end about -->
            <div class="divider"></div>
            <div class="section" id="news">
                <!--   NEWS   -->
                <div class="row ">
                    <div class="col s12 m12 offset-l1 l10">
                        <h4 class="center">News</h4>
                        <!--<h5>News</h5>-->
                        <ul class="pubs_ul">
                            <li>    Busy February! I have spent the last two weeks in Ghana teaching Reinforcerment Learning at <a href="https://aimsammi.org/">AIMS AMMI</a>. It has been a wonderful and enriching experience. Please visit the <a href="https://aimsammi.org/">AMMI website</a> to know more about this very nice initiative.
                                <span class="news_date">01.03.2020</span>
                            </li>
                            <li>
                                I gave a <b>tutorial on exploration-exploitation in RL</b> with M. Ghavamzadeh and A. Lazaric at AAAI'20. You can find the material at this page <a href="https://rlgammazero.github.io/">rlgammazero.github.io</a>.
                                <span class="news_date">20.02.2020</span>
                            </li>
                            <li>
                                1 Paper accepted at AAAI'20 and 1 at AISTATS'20.
                                <span class="news_date">16.01.2020</span>
                            </li>
                            <li>
                                2 Papers accepted at NeurIPS'19.
                                <span class="news_date">10.9.2019</span>
                            </li>
                            <li>
                                I gave a <a href="http://teopir.github.io/files/2019/RLSS_pg.pdf">tutorial on policy gradient and actor-critic</a> at the <a href="https://rlss.inria.fr/">Reinforcement Learning Summer School (RLSS)</a> in Lille. It is always nice to be back in Lille and meet with the amazing people in Sequel! Very well organized summer school.
                                <span class="news_date">15.7.2019</span>
                            </li>
                            <li>
                                Heading to Chicago where, together with <a href="http://ronan.fruit.nom.fr/" target="_blank">Ronan</a> and <a href="http://chercheurs.lille.inria.fr/~lazaric" target="_blank">Alessandro</a>, I will give a tutorial on <i>regret minimization in reinforcement learning</i> at <a href="http://alt2019.algorithmiclearningtheory.org/" target="_blank">ALT'19</a>. Visit out wesite for more info <a href="https://rlgammazero.github.io/">rlgammazero.github.io</a>
                                <span class="news_date">20.3.2019</span>
                            </li>
                            <li>I've been invited to give a talk at <a href="http://awrl.cc/2018.html" target="_blank">ARWL'18</a> in Beijing, China. I will talk about regret minimization (exploration-exploitation) in RL with prior knowledge (<a href="./files/2018/PEB_18.pdf">slides</a>).
                                I've been also invited to give the same talk at MSRA in Beijing.
                                <span class="news_date">6.11.2018</span>
                            </li>
                            <li>Going to NeurIPS! I've received a free registration as one of the <a href="https://neurips.cc/Conferences/2018/Reviewers" target="_blank">"top" reviewers</a>. Moreover, I have one paper accepted at NeurIPS'18. <span class="news_date">29.9.2018</span>
                            </li>
                            <li>
                                I'm really happy to announce that I've been selected for a research position (CR) at INRIA - Lille (<a href="https://www.inria.fr/institut/recrutement-metiers/offres/charge.e.s-recherche-de-classe-normale/admission">link</a>).
                                I've even more happy to announce that I will join Facebook AI Research <i class="fab fa-facebook"></i> (Paris) in October 2018. <span class="news_date">30.7.2018</span>
                            </li>
                            <li>
                                Busy April! I have been giving several talks on exploration-exploitation in RL: Politecnico di Milano (Apr 03), Facebook Paris (Apr 17) and Google Zurich (Apr 27). <span class="news_date">1.6.2018</span>
                            </li>
                            <li>
                                3 papers accepted at ICML'18.
                            </li>
                            <li>
                                I am organizing the <a href="https://ewrl.wordpress.com/ewrl14-2018/">14th European workshop on reinforcement learning (EWRL 2018)</a> <i class="icon-ewrl_logo_3"></i>.
                            </li>
                            <li>
                                ICML/IJCAI workshop on <a href="http://reinforcement-learning.ml/pgmrl2018" target="_blank"><i>Prediction and Generative Modeling in Reinforcement Learning (PGMRL)</i></a>.<br>
                                Organizers: Me,
                                <a href="https://www.robertocalandra.com" target="_blank">Roberto Calandra</a> (UC Berkeley),
                                <a href="https://people.eecs.berkeley.edu/~svlevine/" target="_blank">Sergey Levine</a> (UC Berkeley),
                                <a href="http://ml.informatik.uni-freiburg.de/former/people/riedmiller/info.html" target="_blank">Martin Riedmiller</a> (DeepMind),
                                <a href="http://researchers.lille.inria.fr/~lazaric/" target="_blank">Alessandro Lazaric</a> (Facebook).
                            </li>
                            <li>
                                <a href="http://ronan.fruit.nom.fr/" target="_blank">Ronan Fruit</a> and I are developping a Python library for Exploration-Exploitation in Reinforcement Learning.<br>
                                It is available on <a href="https://github.com/RonanFR/UCRL" target="_blank">GitHub</a>.<br>
                            </li>
                            <li>
                                I'm going to visit Berlin and I'll give a talk at Amazon (Mar 19, 2018) on <i>Efficient Exploration-Exploitation in RL</i>. <span class="news_date">2.3.2017</span>
                            </li>
                            <li>
                                3 papers accepted at NIPS 2017.
                            </li>
                            <li>
                                I'm going to spend a couple of weeks in California.
                                I will visit UC Berkeley and I'll give a talk on <i>Regret Minimization in MDPs with Options</i> (Jul 14, 2017).
                                I will then spend one week at Stanford University. <span class="news_date">1.6.2017</span>
                            </li>
                        </ul>
                    </div>
                </div>
            </div>
            <div class="divider"></div>
            <div class="section" id="pubs">
                <!--   Publications Section   -->
                <div class="row light">
                    <div class="col s12 m12 offset-l1 l10">
                        <h4 class="center">Publications</h4>
                        <h5>Preprints</h5>
                        <ul class="pubs_ul">
                            <li>
                                Yonathan Efroni, Shie Mannor and <span class="pubs_me">Matteo Pirotta</span>:<br>
                                <span class="pubs_title">Exploration-Exploitation in Constrained MDPs.
                                </span> arXiv:2003.02189, 2020. [<a href="https://arxiv.org/abs/2003.02189">arXiv</a>]
                            </li>
                            <li>
                                <span class="pubs_title">Active Model Estimation in Markov Decision Processes</span>
                                arXiv:2003.03297, 2020. [<a href="https://arxiv.org/abs/2003.02189">arXiv</a>]
                            </li>
                            <li>
                                Evrard Garcelon, Baptiste Roziere, Laurent Meunier, Jean Tarbouriech, Olivier Teytaud, Alessandro Lazaric and <span class="pubs_me">Matteo Pirotta</span>:<br>
                                <span class="pubs_title">Adversarial Attacks on Linear Contextual Bandits.
                                </span> arXiv:2002.03839, 2020. [<a href="https://arxiv.org/abs/2002.03839">arXiv</a>]
                            </li>
                            <li>
                                Ronan Fruit, <span class="pubs_me">Matteo Pirotta</span> and Alessandro Lazaric:<br>
                                <span class="pubs_title">Improved Analysis of UCRL2 with empirical Bernstein bounds.
                                </span> ALT Tutorial, 2019. [<a href="https://rlgammazero.github.io/docs/ucrl2b_improved.pdf">arXiv</a>]
                            </li>
                            <li>
                                Jian Qian, Ronan Fruit, <span class="pubs_me">Matteo Pirotta</span> and Alessandro Lazaric:<br>
                                <span class="pubs_title">Concentration Inequalities for Multinoulli Random Variables.
                                </span> ALT Tutorial, 2019. [<a href="https://arxiv.org/abs/2001.11595">arXiv</a>]
                            </li>
                            <li>
                                <span class="pubs_me">Matteo Pirotta</span> and Marcello Restelli:<br>
                                <span class="pubs_title">Cost-Sensitive Approach to Batch Size Adaptation for Gradient Descent.
                                </span> arXiv:1712.03428, 2017.
                            </li>
                        </ul>
                        <h5>Conference Papers</h5>
                        <ul class="pubs_ul">

                            <li>
                                Evrard Garcelon, Mohammad Ghavamzadeh, Alessandro Lazaric and <span class="pubs_me">Matteo Pirotta</span>:<br>
                                <span class="pubs_title">
                                    Conservative Exploration in Reinforcement Learning.
                                </span>
                                AISTATS 2020, Palermo, Italy. [<a href="https://arxiv.org/abs/2002.03218">arXiv</a>]
                            </li>
                            <li>
                                Evrard Garcelon, Mohammad Ghavamzadeh, Alessandro Lazaric and <span class="pubs_me">Matteo Pirotta</span>:<br>
                                <span class="pubs_title">
                                    Improved Algorithms for Conservative Exploration in Bandits.
                                </span>
                                AAAI 2020, New York, USA. [<a href="https://arxiv.org/abs/2002.03221">arXiv</a>]
                            </li>
                            <li>
                                Ronald Ortner, <span class="pubs_me">Matteo Pirotta</span>, Alessandro Lazaric, Ronald Fruit and Odalrici-Ambrym Maillard:<br>
                                <span class="pubs_title">
                                    Regret Bounds for Learning State Representations in Reinforcement Learning.
                                </span>
                                NeurIPS 2019, Vancouver, Canada.
                            </li>
                            <li>
                                Jian Qian, Ronan Fruit, <span class="pubs_me">Matteo Pirotta</span> and Alessandro Lazaric:<br>
                                <span class="pubs_title">
                                    Exploration Bonus for Regret Minimization in Discrete and Continuous Average Reward MDPs.
                                </span>
                                NeurIPS 2019, Vancouver, Canada.
                                [<a href="https://arxiv.org/abs/1812.04363" target="_blank">arXiv</a>] [<a href="https://papers.nips.cc/paper/8735-exploration-bonus-for-regret-minimization-in-discrete-and-continuous-average-reward-mdps">Paper</a>]
                            </li>
                            <li>
                                Ronan Fruit, <span class="pubs_me">Matteo Pirotta</span> and Alessandro Lazaric:<br>
                                <span class="pubs_title">Near Optimal Exploration-Exploitation in Non-Communicating Markov Decision Processes.
                                </span>
                                NeurIPS 2018, Montr&eacute;al, Canada.
                                [<a href="https://arxiv.org/abs/1807.02373" target="_blank">arXiv</a>] [<a href="http://papers.nips.cc/paper/7563-near-optimal-exploration-exploitation-in-non-communicating-markov-decision-processes">Paper</a>]
                            </li>
                            <li>
                                Ronan Fruit, <span class="pubs_me">Matteo Pirotta</span>, Alessandro Lazaric and Ronald Ortner:<br>
                                <span class="pubs_title">Efficient Bias-Span-Constrained Exploration-Exploitation in Reinforcement Learning.
                                </span> ICML 2018, Stockholm, Sweden. [<a href="https://arxiv.org/abs/1802.04020">arXiv</a>]
                            </li>
                            <li>
                                Matteo Papini, Damiano Binaghi, Giuseppe Canonaco, <span class="pubs_me">Matteo Pirotta</span> and Marcello Restelli:<br>
                                <span class="pubs_title">Stochastic Variance-Reduced Policy Gradient.
                                </span> ICML 2018, Stockholm, Sweden. [<a href="https://arxiv.org/abs/1806.05618">arXiv</a>] [<a href="http://proceedings.mlr.press/v80/papini18a.html">Paper</a>]
                            </li>
                            <li>
                                Andrea Tirinzoni, Andrea Sessa, <span class="pubs_me">Matteo Pirotta</span> and Marcello Restelli:<br>
                                <span class="pubs_title">Importance Weighted Transfer of Samples in Reinforcement Learning.
                                </span> ICML 2018, Stockholm, Sweden. [<a href="https://arxiv.org/abs/1805.10886">arXiv</a>] [<a href="http://proceedings.mlr.press/v80/tirinzoni18a.html">Paper</a>]
                            </li>
                            <li>
                                Davide Di Febbo, Emilia Ambrosini, <span class="pubs_me">Matteo Pirotta</span>, Eric Rojas, Marcello Restelli, Alessandra Pedrocchi and Simona Ferrante:<br>
                                <span class="pubs_title">Does Reinforcement Learning Outperform PID in the Control of FES Induced Elbow Flex-Extension?</span> MeMeA 2018, Rome, Italy.
                            </li>
                            <li>Ronan Fruit, <span class="pubs_me">Matteo Pirotta</span>, Alessandro Lazaric, and Emma Brunskill:<br>
                                <span class="pubs_title">Regret Minimization in MDPs with Options without Prior Knowledge.
                                </span>
                                NIPS 2017, Long Beach, California, USA.
                                [<a href="http://fruit.nom.fr/WordPress3/wp-content/uploads/2016/12/poster_options.pdf" target="_blank">Poster</a>]
                                [<a href="https://hal.inria.fr/hal-01649082/" target="_blank">Full Paper</a>]
                            </li>
                            <li>Alberto Metelli, <span class="pubs_me">Matteo Pirotta</span>, and Marcello Restelli:<br>
                                <span class="pubs_title">Compatible Reward Inverse Reinforcement Learning.
                                </span>
                                NIPS 2017, Long Beach, California, USA.
                                [<a href="https://albertometelli.github.io/download/poster_nips2017.pdf" target="_blank">Poster</a>]
                                [<a href="https://papers.nips.cc/paper/6800-compatible-reward-inverse-reinforcement-learning" target="_blank">Paper</a>]
                            </li>
                            <li>Matteo Papini, <span class="pubs_me">Matteo Pirotta</span>, and Marcello Restelli:<br>
                                <span class="pubs_title">Adaptive Batch Size for Safe Policy Gradients.
                                </span>
                                NIPS 2017, Long Beach, California, USA.
                                [<a href="https://t3p.github.io/download/poster_NIPS17.pdf" target="_blank">Poster</a>]
                                [<a href="https://papers.nips.cc/paper/6950-adaptive-batch-size-for-safe-policy-gradients" target="_blank">Paper</a>]
                            </li>
                            <li>Davide Tateo, <span class="pubs_me">Matteo Pirotta</span>, Andrea Bonarini and Marcello Restelli:<br>
                                <span class="pubs_title">Gradient-Based Minimization for Multi-Expert Inverse Reinforcement Learning.
                                </span>
                                IEEE SSCI 2017, Hawaii, USA.
                            </li>
                            <li>Samuele Tosatto, <span class="pubs_me">Matteo Pirotta</span>, Carlo D'Eramo, and Marcello Restelli:<br>
                                <span class="pubs_title">Boosted Fitted Q-Iteration.
                                </span>
                                ICML 2017, Sydney, New South Wales, Australia.
                            </li>
                            <li>Carlo D'Eramo, Alessandro Nuara, <span class="pubs_me">Matteo Pirotta</span>, and Marcello Restelli:<br>
                                <span class="pubs_title">Estimating the Maximum Expected Value in Continuous Reinforcement Learning Problems.
                                </span>
                                AAAI 2017, San Francisco, California, USA.
                            </li>
                            <li><span class="pubs_me">Matteo Pirotta</span>, and Marcello Restelli:<br>
                                <span class="pubs_title">Inverse Reinforcement Learning through Policy Gradient Minimization.
                                </span>
                                AAAI 2016, Phoenix, Arizona, USA.
                            </li>
                            <li><span class="pubs_me">Matteo Pirotta</span>, Simone Parisi, and Marcello Restelli:<br>
                                <span class="pubs_title">Multi-Objective Reinforcement Learning with Continuous Pareto Frontier Approximation.
                                </span>
                                AAAI 2015, Austin, Texas, USA.
                            </li>
                            <li>Caporale Danilo, Luca Deori, Roberto Mura, Alessandro Falsone, Riccardo Vignali, Luca Giulioni,
                                <span class="pubs_me">Matteo Pirotta</span> and Giorgio Manganini:<br>
                                <span class="pubs_title">Optimal Control to Reduce Emissions in Gasoline
                                    Engines: An Iterative Learning Control Approach for ECU Calibration Maps Improvement.
                                </span>
                                ECC 2015, Linz, Austria.
                            </li>
                            <li>Giorgio Manganini, <span class="pubs_me">Matteo Pirotta</span>, Marcello Restelli, Luca Bascetta:<br>
                                <span class="pubs_title">Following Newton
                                    Direction in Policy Gradient with Parameter Exploration.
                                </span>
                                IJCNN 2015, Killarney, Ireland.
                            </li>
                            <li>Simone Parisi, <span class="pubs_me">Matteo Pirotta</span>, Nicola Smacchia, Luca Bascetta, Marcello Restelli:<br>
                                <span class="pubs_title">Policy
                                    Gradient Approaches for Multi-Objective Sequential Decision Making: A Comparison.
                                </span>
                                ADPRL 2014, Orlando, Florida, United States.
                            </li>
                            <li>Simone Parisi, <span class="pubs_me">Matteo Pirotta</span>, Nicola Smacchia, Luca Bascetta and Marcello Restelli:<br>
                                <span class="pubs_title">Policy
                                    Gradient Approaches for Multi-Objective Sequential Decision Making.
                                </span>
                                IJCNN 2014, Beijing, China.
                            </li>
                            <li><span class="pubs_me">Matteo Pirotta</span>, Giorgio Manganini, Luigi Piroddi, Maria Prandini and Marcello Restelli:<br>
                                <span class="pubs_title">A particle-based policy for the optimal control of Markov decision processes.</span>
                                IFAC 2014, Cape Town, South Africa.
                            </li>
                            <li><span class="pubs_me">Matteo Pirotta</span>, Marcello Restelli, Luca Bascetta:<br>
                                <span class="pubs_title">Adaptive Step-Size for Policy Gradient Methods.
                                </span>
                                NIPS 2013, Lake Tahoe, Nevada, USA.
                            </li>
                            <li><span class="pubs_me">Matteo Pirotta</span>, Marcello Restelli, Alessio Pecorino, and Daniele Calandriello:<br>
                                <span class="pubs_title">Safe policy 	iteration.</span>
                                ICML 2013, Atlanta, Georgia, USA. [<a href="http://proceedings.mlr.press/v28/pirotta13.html" target="_blank">Paper</a>]
                            </li>
                            <li>Martino Migliavacca, Alessio Pecorino, <span class="pubs_me">Matteo Pirotta</span>,
                                Marcello Restelli, and Andrea Bonarini:<br>
                                <span class="pubs_title">
                                    Fitted Policy Search.
                                </span>
                                ADPRL 2011, Paris, France.
                            </li>
                            <li>Martino Migliavacca, Alessio Pecorino, <span class="pubs_me">Matteo Pirotta</span>, Marcello Restelli, and Andrea Bonarini:<br>
                                <span class="pubs_title">
                                    Fitted Policy Search: Direct Policy Search using a Batch Reinforcement Learning Approach.
                                </span>
                                ERLARS 2010, Lisboa, Portugal.
                            </li>
                        </ul>
                        <h5>Journal Papers</h5>
                        <ul class="pubs_ul">
                            <li>
                                Simone Parisi, <span class="pubs_me">Matteo Pirotta</span> and Jan Peters:<br>
                                <span class="pubs_title">Manifold-based Multi-objective Policy Search with Sample Reuse.
                                </span>
                                Neurocomputing 263, 2017. [<a href="https://www.sciencedirect.com/science/article/pii/S0925231217310986">Paper</a>]
                            </li>
                            <li>
                                Giorgio Manganini, <span class="pubs_me">Matteo Pirotta</span>, Marcello Restelli, Luigi Piroddi, and
                                Maria Prandini:<br>
                                <span class="pubs_title">Policy search for the optimal control of Markov decision processes:
                                    a novel particle-based iterative scheme.
                                </span>
                                IEEE Transactions on Cybernetics 46, 2016. [<a href="https://ieeexplore.ieee.org/document/7303937/">Paper</a>]
                            </li>
                            <li>
                                Simone Parisi, <span class="pubs_me">Matteo Pirotta</span> and Marcello Restelli:<br>
                                <span class="pubs_title">Multi-objective Reinforcement Learning through Continuous
                                    Pareto Manifold Approximation.
                                </span>
                                Journal of Artificial Intelligence Research 57, 2016. [<a href="https://jair.org/index.php/jair/article/view/11026">Paper</a>]
                            </li>
                            <li>
                                <span class="pubs_me">Matteo Pirotta</span>, Marcello Restelli and Luca Bascetta:<br>
                                <span class="pubs_title">Policy Gradient in Lipschitz Markov Decision Processes.</span>
                                Machine Learning 100, 2015. [<a href="https://link.springer.com/article/10.1007/s10994-015-5484-1">Paper</a>]
                            </li>
                        </ul>
                        <h5>Workshops Papers</h5>
                        <ul class="pubs_ul">
                            <li>Ronan Fruit, <span class="pubs_me">Matteo Pirotta</span>, Alessandro Lazaric and Emma Brunskill:<br>
                                <span class="pubs_title">
                                    Regret Minimization in MDPs with Options without Prior Knowledge.
                                </span>
                                <a href="http://rlabstraction2016.wixsite.com/icml-2017">Lifelong Learning: A Reinforcement Learning Approach</a>, ICML 2017 Workshop, Sydney, Australia.
                                [<a href="https://drive.google.com/file/d/0B9dqzboiV5u-Z2h0cHVWa2JwWUk/view">PDF</a>, <a href="./files/2017/ICML_LLL_options.pdf">PDF2</a>]
                            </li>
                            <li><span class="pubs_me">Matteo Pirotta</span>, and Marcello Restelli:<br>
                                <span class="pubs_title">
                                    Cost-Sensitive Approach for Batch Size Optimization.
                                </span>
                                <a href="http://probabilistic-numerics.org/meetings/NIPS2016/">Optimizing the optimizers</a>, NIPS 2016 Workshop, Barcelona, Spain.
                                [<a href="http://probabilistic-numerics.org/assets/pdf/NIPS2016/Pirotta_Restelli.pdf">PDF</a>]
                            </li>
                        </ul>
                    </div>
                </div>
            </div><!-- section pubs -->
            <div class="divider"></div>
            <div class="section" id="teaching">
                <!--   Teaching Section   -->
                <div class="row light">
                    <div class="col s12 m12 offset-l1 l10">
                        <h4 class="center">Teaching</h4>
                        <h5>Reinforcement Learning - Spring 2019 - African Masterâ€™s in Machine Intelligence (AMMI) - Ghana</h5>
                        <ul class="pubs_ul">
                            <li>Piazza: Registration (with your school email) and online class discussion on <a href="https://piazza.com/ens-cachan.fr/spring2020/ammirl">piazza</a></li>
                        </ul>
                        <h5>Reinforcement Learning - Fall 2018 - MVA - ENS Paris-Saclay</h5>
                        <ul class="pubs_ul">
                            <li>Piazza: Registration (with your school email) and online class discussion on <a href="https://piazza.com/ens-cachan.fr/spring2018/mvarl/home">piazza</a></li>
                        </ul>
                        <h5>Previous Classes</h5>
                        <ul class="pubs_ul">
                            <li>Reinforcement Learning - Fall 2017 - MVA - ENS Paris-Saclay</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div><!-- container -->

        <footer class="page-footer blue darken-3" id="foot">
            <div class="container">
                <div class="row">
                    <div class="col offset-l1 l6 s8">
                        <h5 class="white-text">Office</h5>
                        <p class="grey-text text-lighten-4">
                            Facebook AI Research<br>
                            75002 Paris, France
                        </p>
                    </div>
                    <div class="col offset-l2 l3 s4">
                        <h5 class="white-text">Links</h5>
                        <ul>
                            <li><a class="white-text" target="_blank" href="https://it.linkedin.com/in/matteo-pirotta-4593a994">Linkedin</a></li>
                            <li><a class="white-text" target="_blank" href="https://scholar.google.com/citations?user=6qWcDTAAAAAJ&hl=en">Google Scholar</a></li>
                            <li><a class="white-text" target="_blank" href="http://arxiv.org/find/grp_cs/1/au:+pirotta_matteo/0/1/0/all/0/1">Arxiv</a></li>
                        </ul>
                    </div>
                </div>
            </div>
            <div class="footer-copyright">
                <div class="container">
                    <div class="row">
                        <div class="col offset-l1 l5 s6">
                            <p class="left">
                                Powered by <a class="grey-text text-lighten-3" href="http://materializecss.com">Materialize</a>
                            </p>
                        </div>
                        <div class="col l5 s6">
                            <p class="right">
                                Hosted on <a class="grey-text text-lighten-4" href="https://pages.github.com/">GitHub Pages</a>
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </footer>


        <!--Import jQuery before materialize.js-->
        <script src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
        <script src="js/materialize.min.js"></script>

        <script src="js/init.js"></script>
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    </body>
</html>
